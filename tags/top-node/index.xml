<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>top-node on Lucius | Braindump</title>
    <link>https://luciuschen.github.io/tags/top-node/</link>
    <description>Recent content in top-node on Lucius | Braindump</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Jun 2022 17:25:46 +0800</lastBuildDate><atom:link href="https://luciuschen.github.io/tags/top-node/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Distribution System</title>
      <link>https://luciuschen.github.io/posts/main/20220602202933-distribution_system/</link>
      <pubDate>Fri, 17 Jun 2022 17:25:46 +0800</pubDate>
      
      <guid>https://luciuschen.github.io/posts/main/20220602202933-distribution_system/</guid>
      <description>Remote Procedure Call Remote Procedure Call (RPC) 使得调用远程函数就像调用本地函数一样，这样就不需要关心资源的具体问题，实际上 RPC 是将方法调用转换成了网络通信。但是遇到通信失败怎</description>
      <content:encoded><![CDATA[<h2 id="remote-procedure-call">Remote Procedure Call</h2>
<p><a href="/posts/main/20220603220847-remote_procedure_call_rpc/">Remote Procedure Call (RPC)</a> 使得调用远程函数就像调用本地函数一样，这样就不需要关心资源的具体问题，实际上 RPC 是将方法调用转换成了网络通信。但是遇到通信失败怎么办？</p>
<ol>
<li>如果在函数调用过程中，服务挂了？</li>
<li>如果消息丢失了？</li>
<li>如果消息延迟了？</li>
<li>如果出了问题，回复消息是否安全？</li>
</ol>
<p>要解决这些问题，就需要将网络以及节点之间的情况抽象出来，以及时序的问题。在设计分布式算法时，需要更加抽象一点，以便更好的推理整个过程。</p>
<h2 id="system-model">System model</h2>
<h3 id="network-behaviour">Network behaviour</h3>
<p><a href="/posts/main/20220602202247-the_two_generals_problem/">The two generals problem</a> 描述了网络通信丢失的场景。</p>
<p>假设两个节点之是间双向的点对点的通信。</p>
<h4 id="reliable-links">Reliable links</h4>
<p>一条消息只会在被一个节点发送的时候才会收到，并且消息是可以重新排序的。</p>
<p>中间没有伪造消息等行为，发送了就一定会被收到。</p>
<h4 id="fair-loss-links">Fair-loss links</h4>
<p>消息可能会丢失、重复或重新排序，如果你一直重新发送，消息终将会传递过去。</p>
<p>每次发送消息成功的概率都是非零，这个时候只要一直重试，我们就假设消息一定会传递到另外一个节点。</p>
<p><strong>Fair-losl links 其实可以通过不断的重试以及并且对重复收到的消息去重，就可以达到 Reliable links 的效果。</strong></p>
<h4 id="arbitrary-links">Arbitrary links</h4>
<p>可能传递过程中会有恶意的第三方对消息进行干预。</p>
<p>比如公共场合的公共 WiFi 就会出现这种情况。</p>
<p><strong>Arbitrary links 通过 TLS 的加持，就可以达到 Fair-loss links 的效果。</strong></p>
<h3 id="nodes-behaviour">Nodes behaviour</h3>
<p><a href="/posts/main/20220602202317-the_byzantine_generals_problem/">The Byzantine generals problem</a> 描述了节点出错，导致全体协作策略失败以及容错率的问题。</p>
<p>假设每个节点都执行指定的算法，假设有以下情况。</p>
<h4 id="crash-stop--fail-stop">Crash-stop (fail-stop)</h4>
<p>如果一个节点出现问题挂了，并且在这之后永远停机。</p>
<h4 id="crash-recovery--fail-recovery">Crash-recovery (fail-recovery)</h4>
<p>一个节点挂了，并丢失了内存状态，但会在过段时间后恢复。</p>
<h4 id="byzantine-fail-arbitrary">Byzantine （fail-arbitrary)</h4>
<p>节点偏离了原本的算法，可能会发生任何事情，包括崩溃或者恶意的行为。</p>
<h3 id="synchrony--timing--assumption">Synchrony (timing) assumption</h3>
<h4 id="synchronous">Synchronous</h4>
<p>消息延迟不超过已知的上限。到达这个上限后，消息要么被传递，要么丢失。节点以已知的速度执行算法。每一步代码的执行都有执行时间的上限。</p>
<h4 id="partically-synchronous">Partically synchronous</h4>
<p>系统中一部分时间内是异步的，另一部分是同步的。</p>
<h4 id="asynchronous">Asynchronous</h4>
<p>消息可以任意地被延迟，节点可以任意地暂停，并没有时序上的保证。</p>
<h2 id="clocks-and-time-in-distributed-systems">Clocks and time in distributed systems</h2>
<p>分布式系统中经常涉及到需要测量时间，比如：</p>
<ul>
<li>Schedulers, timeouts, failure detectors, retry timers</li>
<li>Performance measurements, statistics, profiling (performance analysis也称为profiling)</li>
<li>Log files &amp; databases: record when an event occurred</li>
<li>Data with time-limited validity (e.g. cache entries)</li>
<li><strong>Determining order of events across several nodes</strong></li>
</ul>
<p>分布式系统中会遇到两种类型的时钟：</p>
<ul>
<li>physical clocks: count number of seconds elapsed</li>
<li>logical clocks: count events, e.g. messages sent</li>
</ul>
<h3 id="clock-synchronisation">Clock synchronisation</h3>
<p>物理时钟会有时间上的偏移，通常是通过和电脑、手机、电视等相对准确的地方来校准物理时钟的时间，比如手表，老式的钟摆。</p>
<p>电脑上的时间同样会有偏移，是通过 <a href="/posts/main/20220602202411-network_time_protocol_ntp/">Network Time Protocol(NTP)</a> 来校准电脑的时间，也就是物理时钟的校准。</p>
<h2 id="broadcast-protocols-and-logical-time">Broadcast protocols and logical time</h2>
<figure>
    <img loading="lazy" src="/ox-hugo/ordering-of-messages.png"
         alt="Figure 1: ordering-of-messages"/> <figcaption>
            <p><span class="figure-number">Figure 1: </span>ordering-of-messages</p>
        </figcaption>
</figure>

<p>\(m_{1}\) = &ldquo;A says: The moon is made of cheese!&rdquo;</p>
<p>\(m_{2}\) = &ldquo;B says: Oh no it isn&rsquo;t!&rdquo;</p>
<p>User C sees \(m_{2}\) first, \(m_{1}\) second, even though logically \(m_{1}\) <strong>happened before</strong> \(m_{2}\).</p>
<p><strong>Problem</strong>: even with synced clocks, \(t_{2 }&lt; t_{1}\) is possible. Timestamp order is inconsistent with expected order!</p>
<p>即便我们已经通过 NTP 尽量保证物理时钟上的同步，但这个时候依旧会发生因果关系不一致的情况，所以这个时候就需要逻辑时钟。</p>
<p>逻辑时钟是在分布式系统中专门用于捕获系统中发生事件之间的因果关系，因此逻辑时钟其实就是一种计数器，每次事件发生的时候都会往前递增，所以随着事件的发生它会随着时间向前移动，但它和物理时间并没有实际的关系。</p>
<h3 id="logical-time">Logical time</h3>
<p>Logical clocks: designed to <strong>capture causal dependencies</strong>.</p>
<p>\((e_{1} \rightarrow e_{2}) \Longrightarrow (T(e_{1}) &lt; T(e_{2}))\)<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>当事件1发生在事件2之前，那么事件1的时间戳应该小于事件2的时间戳，这是逻辑时钟要保证的最基本的原则。</p>
<p>下面将研究一下两种逻辑时钟：</p>
<ul>
<li>
<p><a href="/posts/main/20220609201658-lamport_clocks/">Lamprot clocks</a> 是在分布式系统中的每个节点都维护一个计数器，每一个当前节点事件 \(e\) 发生时自增 ；发送消息时，也会附带上当前节点的计数 \(t\) ，消息接收者收到消息后，取出消息中的计数器与当前节点计数器比较，去最大值后自增 1。</p>
<p>这样可以实现局部顺序，再加上节点名称，形成(时间戳，节点名称)的组合后，就可以扩展为全局顺序。</p>
<p>但这样也有缺点，算法在时间戳相同，节点名称不同的平行事件的顺序判定时，会出现与实际情况不符的情况。<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
</li>
<li>
<p><a href="/posts/main/20220602235916-vector_clocks/">Vector clocks</a> 除了是向量之外，向量时钟算法与 Lamport 时钟非常相似。一个节点的向量时钟的初始值是系统中的每个节点的事件数，也就是 0。每当节点 \(N_{i}\) 发生事件时，它就会增加其向量钟中的第 \(i\) 个条目（它自己的条目）。(In practice, this vector is often implemented as a map from node IDs to integers rather than an array of integers. 在实践中，这里的向量通常是节点 ID 对应 Integer 的 map，而不是 Integer 的数组。)。当一个消息在网络上被发送时，发送者当前的向量时间戳被附加到该消息上。最后，当一个消息被接收时，接收者将消息中的向量时间戳与它的本地时间戳合并，方法是取两个向量的元素的最大值，然后接收者增加它自己的条目</p>
<p>这样就可以弥补 Lamport 的缺点，可以完全确定每个事件的因果顺序。</p>
</li>
</ul>
<h3 id="delivery-order-in-broadcast-protocols">Delivery order in broadcast protocols</h3>
<p>许多网络提供点对点（单播）的信息传递，其中一个信息有一个特定的收件人。广播协议对网络进行了概括，使一条信息被发送到某个组的所有节点。组的成员可能是固定的，或者系统可能提供节点加入和离开组的机制。</p>
<p>一些局域网在硬件层面提供组播或广播（例如，IP 组播），但互联网上的通信通常只允许单播。此外，硬件级的组播通常是在 best-effort 基础上提供的，允许消息被丢弃；要使其可靠，需要类似于这里讨论的重传协议。</p>
<p>上面提到的 node behaviour 和 synchrony 的系统模型假设直接延伸到广播组。</p>
<h4 id="broadcast-protocols">Broadcast protocols</h4>
<p>Broadcast (multicast) is <strong>group communication</strong>:</p>
<ul>
<li>一个节点发送消息，所有组内节点传递。</li>
<li>组内成为可以是固定的也可以是动态的</li>
<li>如果一个节点发生错误，剩余的组内成员顶上。</li>
<li>Note: 这个观念比 IP 组播更加普遍</li>
</ul>
<p>建立在之前讲到的系统模型上：</p>
<ul>
<li>可以是 <strong>best-effort</strong> (可能会丢失消息) 或者 <strong>reliable</strong> (非故障节点传递每条消息，重传丢失的消息。)</li>
<li>Asynchronous / partially synchronous timing model \(\Longrightarrow\) 消息延迟​<strong>没有上限</strong></li>
</ul>
<h4 id="receiving-versus-delivering">Receiving versus delivering</h4>
<figure>
    <img loading="lazy" src="/ox-hugo/receive-versus-deliver.png"
         alt="Figure 2: receive-versus-deliver"/> <figcaption>
            <p><span class="figure-number">Figure 2: </span>receive-versus-deliver</p>
        </figcaption>
</figure>

<p>假定网络提供点对点的发送/接收，在广播算法​从网络中​<strong>收到</strong>​消息后，传递给应用前会在缓存或者队列当中。</p>
<p>下面将研究三种不同形式的「广播」。所有这些都是可靠的：每条消息最终都会被传递到每个非故障节点，但没有时间上的保证。然而，它们在每个节点上传递信息的顺序方面存在差异。事实证明，这种顺序上的差异对实现广播的算法有非常根本的影响。</p>
<h4 id="forms-of-reliable-broadcast">Forms of reliable broadcast</h4>
<ul>
<li>
<p><a href="/posts/main/20220616105609-fifo_broadcast/">FIFO broadcast</a></p>
<p>如果 \(m_{1}\) 和 \(m_{2}\) 从相同的节点广播，并且 broadcast(\(m_{1}\)) \(\longrightarrow\) broadcast(\(m_{2}\))，那么 \(m_{1}\) 必然在 \(m_{2}\) 之前被传递。</p>
</li>
<li>
<p><a href="/posts/main/20220616132407-causal_broadcast/">Causal broadcast</a></p>
<p>如果 broadcast(\(m_{1}\)) \(\longrightarrow\) broadcast(\(m_{2}\)) 那么 \(m_{1}\) 必然在 \(m_{2}\) 之前被传递。</p>
</li>
<li>
<p><a href="/posts/main/20220616135058-total_order_broadcast/">Total order broadcast</a></p>
<p>如果在同一个节点 \(m_{1}\) 在 \(m_{2}\) 之前被传递，那么所有节点 \(m_{1}\) 必然在 \(m_{2}\) 之前被传递。</p>
</li>
<li>
<p><strong>FIFO-total order broadcast</strong></p>
<p>是 FIFO broadcast 和 Total order broadcast 的结合</p>
</li>
</ul>
<h4 id="relationships-between-broadcast-models">Relationships between broadcast models</h4>
<figure>
    <img loading="lazy" src="/ox-hugo/broadcast-models-relationship.png"
         alt="Figure 3: broadcast-models-relationship"/> <figcaption>
            <p><span class="figure-number">Figure 3: </span>broadcast-models-relationship</p>
        </figcaption>
</figure>

<p>FIFO-total order broadcast 是一个严格意义上比因果广播更强的模型；换句话说，每个有效的 FIFO-total order broadcast 协议也是一个有效的 causal broadcast 协议（反过来就不是了），其他协议也是如此。</p>
<h3 id="broadcast-algorithms">Broadcast algorithms</h3>
<p>实现广播算法，简单来说，涉及到两个步骤。</p>
<ol>
<li>确保每个节点都能收到每天消息</li>
<li>以正确的顺序传递这些消息</li>
</ol>
<p>首先研究如何可靠地传递消息，当一个节点想要广播一条消息时，它就单独向其他每个节点发送消息，使用上面提到过的 reliable links。但很可能一条消息丢失，发送的节点在重新发送前就崩溃了，这种情况下，丢失的这条消息对应的节点就无法接收到这条消息。</p>
<p>为了提高可靠性，可以让其他节点帮忙。例如，当一个节点第一次收到一个特定消息时，它就把消息转发给其他每个节点（这种被称为急性可靠广播（Eager reliable broadcast））。这种算法确保了即使一些节点崩溃，所有剩下的非故障的节点都会收到每条消息。然而，这种算法很低效：在没有故障的情况下，每条消息在由 \(n\) 个节点组成的小组中要发送 \(O(n^{2})\) 次，每个节点要收到每条消息 \(n-1\) 次，意味着占用了大量的带宽。</p>
<p>eager reliable broadcast 的变体沿着不同维度（比如容错性、所有节点接收消息的时间和被使用的带宽）优化，其中最常见的是流言协议（Gossip protocols，也被叫做流行病协议（Epidemic protocols））。在这些协议中，一个想广播消息的节点将其发送给随机选择的少量固定数量的节点。第一次收到信息时，一个节点会将其转发给固定数量的随机选择的节点。这类似于流言、谣言或传染病在人群中的传播方式。</p>
<p>Gossip protocols 并不能保证所有节点都能收到消息：在随机选择节点时，有可能总是遗漏了一些节点。然而，如果算法的参数选择得当，信息不被传递的概率很小。Gossip protocols 很有吸引力，在正确的参数下，对消息丢失和节点崩溃有很强的弹性，同时还能保持高效。其实核裂变的发生过程，也类似于这种协议。</p>
<p>可以在 eager reliable broadcast 或 gossip protocol 的基础上，建立 FIFO、causal 或者 total order broadcast。</p>
<ul>
<li><a href="/posts/main/20220616105609-fifo_broadcast/#fifo-broadcast-algorithm">FIFO broadcast algorithm</a></li>
<li><a href="/posts/main/20220616132407-causal_broadcast/#causal-broadcast-algorithm">Causal broadcast algorithm</a></li>
<li><a href="/posts/main/20220616135058-total_order_broadcast/#total-order-broadcast-algorithms">Total order broadcast algorithms</a></li>
</ul>
<h2 id="replication">Replication</h2>
<p>现在要讨论的是复制的问题，也就是在多个节点上维护相同数据的副本，每个节点被称为副本。复制是许多分布式数据库、文件系统和其他存储系统的一个标准特征。它是我们实现容错的主要机制之一：如果一个副本出现故障，我们可以继续访问其他副本上的数据副本。</p>
<ul>
<li>在不同节点维护相同数据的副本</li>
<li>数据库、文件系统、缓存等等</li>
<li>维护相同数据的节点叫做副本</li>
<li>如果一些副本出错，其他副本依旧是可访问的。</li>
<li>将负载分散到多个副本</li>
<li>如果数据没有变化，维护很简单，只需要复制。</li>
<li>专注于数据变化</li>
</ul>
<p>相较于 <a href="https://zh.wikipedia.org/zh-cn/RAID">RAID</a> (Redundant Array of Independent Disks)：在单个计算机内复制。</p>
<ul>
<li>RAID 只有一个控制器；在分布式系统中，每个节点都是独立的。</li>
<li>副本可以在全世界靠近用户的地方分布（CDN 本质其实是一种大规模分布式多级缓存系统）</li>
</ul>
<h3 id="manipulating-remote-state">Manipulating remote state</h3>
<p>在更新不同节点数据时，丢失数据或者丢失 TCP 返回的 \(ACK\)<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> 时，就会使得不同节点之间的副本不一致，不满足幂等性（Idempotent）。为了解决这个问题，可以给每次更新操作都加上一个逻辑时间戳，并将其作为数据存储在数据库当中；当需要删除这条记录时，实际上并没有删除，而是写入一个特殊的值（称为墓碑 tombstone），将其标记为删除。（其实就类似于软删除）</p>
<figure>
    <img loading="lazy" src="/ox-hugo/reconciling-replicas.png"
         alt="Figure 4: reconciling-replicas"/> <figcaption>
            <p><span class="figure-number">Figure 4: </span>reconciling-replicas</p>
        </figcaption>
</figure>

<p>在许多副本系统中，副本运行一个协议来检测和调和任何差异（这被称为反熵 anti-entropy），以便副本最终持有相同数据的一致副本。由于有了 tombstone，反熵过程可以分辨出已经被删除的记录和尚未被创建的记录之间的差异。由于有了逻辑时间戳，可以分辨出一条记录的哪个版本比较老，哪个版本比较新。然后，反熵过程会保留较新的记录并丢弃较旧的记录。</p>
<figure>
    <img loading="lazy" src="/ox-hugo/concurrent-reconciling-replicas.png"
         alt="Figure 5: concurrent-reconciling-replicas"/> <figcaption>
            <p><span class="figure-number">Figure 5: </span>concurrent-reconciling-replicas</p>
        </figcaption>
</figure>

<p>如果同时两个请求分别想把两个副本的 \(x\) 的值进行修改。</p>
<ul>
<li>
<p><strong>Last writer wins</strong> (LWW):</p>
<p>采用 Lamport clocks 时，\(t_{2} \gt t_{1}\)则保留 \(v_{2}\) 丢弃 \(v_{1}\)。</p>
</li>
<li>
<p><strong>Multi-value register</strong>:</p>
<p>采用 Vector clocks 时，\(t_{2 }\gt t_{1}\) 则保留 \(v_{2}\) 丢弃 \(v_{1}\); \(t_{2} \rVert t_{1}\) 则 \(\{v_{1}, v_{2}\}\) 都保留。</p>
</li>
</ul>
<p>向量时钟的缺点是它们会变得很昂贵：每个客户端都需要在向量中输入一个条目，在有大量客户端的系统中（或者客户端每次重启都会有新的身份），这些向量会变得很大，可能会比数据本身占用更多内存。更多类型的逻辑时钟，如点状版本向量（Dotted version vectors）<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>，已经被开发出来以优化这种类型的系统。</p>
<h3 id="quorums">Quorums</h3>
<p>具体如何进行复制的细节对系统的可靠性有很大影响。如果没有容错，拥有多个副本会使可靠性变差：副本越多，任何一个副本在任何时候出现故障的概率就越大（假设故障不是完全相关的）。然而，如果系统在一些有问题的复制体中仍然继续工作，那么可靠性就会提高：所有副本在同一时间出现问题的概率要比一个复制体出现问题的概率低很多。</p>
<h4 id="read-after-write-consistency">Read-after-write consistency</h4>
<figure>
    <img loading="lazy" src="/ox-hugo/read-after-write-consistency.png"
         alt="Figure 6: read-after-write-consistency"/> <figcaption>
            <p><span class="figure-number">Figure 6: </span>read-after-write-consistency</p>
        </figcaption>
</figure>

<p>一个副本写入了数据，从另外一个副本读取，用户并没有读取到他刚刚提交的内容。如果要保持读写一致性（read-after-write consistency），则要两个节点都同时写入和读取，一旦有一个不可访问就会破坏读写一致性，没有任何容错率可言。</p>
<h4 id="quorum--2-out-of-3">Quorum (2 out of 3)</h4>
<p>通过使用三个副本来解决这个难题，。我们将每个读写请求发送到所有三个副本，但只要收到 \(≥2\) 个响应，我们就认为请求成功。在这个例子中，写在复制体B和C上成功，而读在复制体A和B上成功。对读和写都采用 &ldquo;三选二 &ldquo;的策略，可以保证至少有一个对读的响应来自看到最近写的复制体（在这个例子中，这是复制体B）。</p>
<figure>
    <img loading="lazy" src="/ox-hugo/quorum.png"
         alt="Figure 7: quorum"/> <figcaption>
            <p><span class="figure-number">Figure 7: </span>quorum</p>
        </figcaption>
</figure>

<p>1</p>
<h2 id="reference">Reference</h2>
<ul>
<li>Cachin, Guerraoui, Rodrigues (2011) Introduction to Reliable and Secure Distributed Programming (2. Ed.)., Springer.</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="/posts/main/20220602202832-causality_and_happen_before_relation/">Causality and Happen-before relation</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>if \(a \rVert b\) we could have either \(a ≺ b\) or \(b ≺ a\), so the order of the two events is determined arbitrarily by the algorithm.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>后面整理 TCP 相关内容时，可以加个链接。&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>待学习&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Refactoring Code</title>
      <link>https://luciuschen.github.io/posts/main/20220506193054-refactoring_code/</link>
      <pubDate>Thu, 16 Jun 2022 09:33:31 +0800</pubDate>
      
      <guid>https://luciuschen.github.io/posts/main/20220506193054-refactoring_code/</guid>
      <description>Refactoring Code How To Find Time Complexity Of An Algorithm
JavaScript Refactoring Tips Refactoring Condition statements Converting callbacks to promises Refactoring Promise chains with async/await Refactoring Code examples Callback Hell </description>
      <content:encoded><![CDATA[<h2 id="refactoring-code">Refactoring Code</h2>
<p><a href="/posts/main/20220506193205-how_to_find_time_complexity_of_an_algorithm/">How To Find Time Complexity Of An Algorithm</a></p>
<h3 id="javascript">JavaScript</h3>
<h4 id="refactoring-tips">Refactoring Tips</h4>
<ol>
<li><a href="/posts/main/20220506193520-refactoring_condition_statements/">Refactoring Condition statements</a></li>
<li><a href="/posts/main/20220506193545-converting_callbacks_to_promises/">Converting callbacks to promises</a></li>
<li><a href="/posts/main/20220506193624-refactoring_promise_chains_with_async_await/">Refactoring Promise chains with async/await</a></li>
</ol>
<h4 id="refactoring-code-examples">Refactoring Code examples</h4>
<ul>
<li><a href="/posts/main/20220616092944-callback_hell/">Callback Hell</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>MySQL</title>
      <link>https://luciuschen.github.io/posts/main/20220530185321-mysql/</link>
      <pubDate>Sat, 11 Jun 2022 11:51:16 +0800</pubDate>
      
      <guid>https://luciuschen.github.io/posts/main/20220530185321-mysql/</guid>
      <description>Troubleshoot Issues in MySQL
Sort Chinese text fields alphabetically by first letter</description>
      <content:encoded><![CDATA[<p><a href="/posts/main/20220530180443-troubleshoot_issues_in_mysql/">Troubleshoot Issues in MySQL</a></p>
<p><a href="/posts/main/20220601104443-sort_chinese_text_fields_alphabetically_by_first_letter/">Sort Chinese text fields alphabetically by first letter</a></p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
